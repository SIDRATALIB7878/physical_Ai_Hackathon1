{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"mainSidebar":[{"type":"category","label":"Foundations","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/docs/foundations/physical_ai","label":"Physical AI: A Primer","docId":"foundations/physical_ai","unlisted":false},{"type":"link","href":"/docs/foundations/diagrams","label":"Conceptual Diagrams for Book Foundations","docId":"foundations/diagrams","unlisted":false},{"type":"link","href":"/docs/foundations/overview","label":"Foundations: Overview","docId":"foundations/overview","unlisted":false}],"href":"/docs/category/foundations"},{"type":"link","href":"/docs/intro","label":"Introduction","docId":"intro","unlisted":false},{"type":"category","label":"Systems","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/docs/systems/ros2_intro","label":"Introduction to ROS 2","docId":"systems/ros2_intro","unlisted":false},{"type":"link","href":"/docs/systems/overview","label":"Systems: Overview","docId":"systems/overview","unlisted":false},{"type":"link","href":"/docs/systems/urdf_control","label":"URDF Model Creation and Basics","docId":"systems/urdf_control","unlisted":false}],"href":"/docs/category/systems"},{"type":"category","label":"Tutorial - Basics","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/docs/tutorial-basics/create-a-page","label":"Create a Page","docId":"tutorial-basics/create-a-page","unlisted":false},{"type":"link","href":"/docs/tutorial-basics/create-a-document","label":"Create a Document","docId":"tutorial-basics/create-a-document","unlisted":false},{"type":"link","href":"/docs/tutorial-basics/create-a-blog-post","label":"Create a Blog Post","docId":"tutorial-basics/create-a-blog-post","unlisted":false},{"type":"link","href":"/docs/tutorial-basics/markdown-features","label":"Markdown Features","docId":"tutorial-basics/markdown-features","unlisted":false},{"type":"link","href":"/docs/tutorial-basics/deploy-your-site","label":"Deploy your site","docId":"tutorial-basics/deploy-your-site","unlisted":false},{"type":"link","href":"/docs/tutorial-basics/congratulations","label":"Congratulations!","docId":"tutorial-basics/congratulations","unlisted":false}],"href":"/docs/category/tutorial---basics"},{"type":"category","label":"Simulation","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"examples","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/docs/simulation/examples/unity_humanoid_project/","label":"This file serves as a placeholder for a Unity project for humanoid simulation.","docId":"simulation/examples/unity_humanoid_project/README","unlisted":false}]},{"type":"link","href":"/docs/simulation/gazebo_basics","label":"Gazebo Simulation Setup","docId":"simulation/gazebo_basics","unlisted":false},{"type":"link","href":"/docs/simulation/overview","label":"Simulation: Overview","docId":"simulation/overview","unlisted":false},{"type":"link","href":"/docs/simulation/unity_robotics","label":"Unity Robotics Simulation Setup","docId":"simulation/unity_robotics","unlisted":false}],"href":"/docs/category/simulation"},{"type":"category","label":"Tutorial - Extras","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/docs/tutorial-extras/manage-docs-versions","label":"Manage Docs Versions","docId":"tutorial-extras/manage-docs-versions","unlisted":false},{"type":"link","href":"/docs/tutorial-extras/translate-your-site","label":"Translate your site","docId":"tutorial-extras/translate-your-site","unlisted":false}],"href":"/docs/category/tutorial---extras"},{"type":"category","label":"AI Perception & Navigation","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/docs/ai_perception_navigation/isaac_ros_pipelines","label":"Isaac ROS Pipelines for Perception and Navigation","docId":"ai_perception_navigation/isaac_ros_pipelines","unlisted":false},{"type":"link","href":"/docs/ai_perception_navigation/isaac_sim_overview","label":"Isaac Sim Overview and Basic Setup","docId":"ai_perception_navigation/isaac_sim_overview","unlisted":false},{"type":"link","href":"/docs/ai_perception_navigation/overview","label":"AI Perception & Navigation: Overview","docId":"ai_perception_navigation/overview","unlisted":false}],"href":"/docs/category/ai-perception--navigation"},{"type":"category","label":"Vision-Language-Action","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/docs/vision_language_action/multimodal_interaction","label":"Multimodal Interaction Strategies","docId":"vision_language_action/multimodal_interaction","unlisted":false},{"type":"link","href":"/docs/vision_language_action/overview","label":"Vision-Language-Action (VLA): Overview","docId":"vision_language_action/overview","unlisted":false},{"type":"link","href":"/docs/vision_language_action/whisper_llm","label":"Whisper and LLMs for Conversational Robotics","docId":"vision_language_action/whisper_llm","unlisted":false}],"href":"/docs/category/vision-language-action"},{"type":"category","label":"Capstone","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/docs/capstone/autonomous_humanoid","label":"Capstone Project: Autonomous Humanoid - Fetch a Beverage","docId":"capstone/autonomous_humanoid","unlisted":false},{"type":"link","href":"/docs/capstone/overview","label":"Capstone: Autonomous Humanoid - Overview","docId":"capstone/overview","unlisted":false}],"href":"/docs/category/capstone"},{"type":"link","href":"/docs/agent_sdk_docs","label":"OpenAI Agents SDK - Python","docId":"agent_sdk_docs","unlisted":false},{"type":"link","href":"/docs/contributing","label":"Contributing to the Book: Citation Guidelines","docId":"contributing","unlisted":false},{"type":"link","href":"/docs/pipeline-guide","label":"Book Content Embeddings Pipeline Guide","docId":"pipeline-guide","unlisted":false}]},"docs":{"agent_sdk_docs":{"id":"agent_sdk_docs","title":"OpenAI Agents SDK - Python","description":"The OpenAI Agents SDK is a lightweight yet powerful framework for building multi-agent workflows in Python. It provides a provider-agnostic approach to orchestrating LLM-based agents, supporting the OpenAI Responses and Chat Completions APIs, as well as 100+ other LLM providers through integrations like LiteLLM. The SDK is designed to handle complex agent interactions with minimal boilerplate code, enabling developers to focus on building intelligent, tool-enabled agents rather than managing low-level API calls and conversation history.","sidebar":"mainSidebar"},"ai_perception_navigation/isaac_ros_pipelines":{"id":"ai_perception_navigation/isaac_ros_pipelines","title":"Isaac ROS Pipelines for Perception and Navigation","description":"This chapter introduces NVIDIA Isaac ROS, a collection of GPU-accelerated ROS 2 packages that provide high-performance solutions for common robotics tasks, especially in perception and navigation. We will explore key Isaac ROS modules and conceptualize how to build efficient pipelines for humanoid robots.","sidebar":"mainSidebar"},"ai_perception_navigation/isaac_sim_overview":{"id":"ai_perception_navigation/isaac_sim_overview","title":"Isaac Sim Overview and Basic Setup","description":"This chapter introduces NVIDIA Isaac Sim, a powerful robotics simulation platform built on Omniverse. We will cover its installation, basic configuration, environment creation, and robot integration, with a focus on preparing for perception and navigation tasks.","sidebar":"mainSidebar"},"ai_perception_navigation/overview":{"id":"ai_perception_navigation/overview","title":"AI Perception & Navigation: Overview","description":"Learning Objectives","sidebar":"mainSidebar"},"capstone/autonomous_humanoid":{"id":"capstone/autonomous_humanoid","title":"Capstone Project: Autonomous Humanoid - Fetch a Beverage","description":"This chapter presents the culminating capstone project: developing an autonomous humanoid robot capable of understanding a high-level natural language command and executing a complex sequence of actions to \"fetch a beverage from a simulated fridge.\" This project integrates concepts and technologies from all previous chapters, including ROS 2, Isaac Sim, AI perception, navigation, and Vision-Language-Action (VLA) pipelines.","sidebar":"mainSidebar"},"capstone/overview":{"id":"capstone/overview","title":"Capstone: Autonomous Humanoid - Overview","description":"Learning Objectives","sidebar":"mainSidebar"},"contributing":{"id":"contributing","title":"Contributing to the Book: Citation Guidelines","description":"This document outlines the guidelines for managing and formatting citations within the \"Physical AI & Humanoid Robotics Capstone Project\" book, adhering strictly to APA 7th edition style.","sidebar":"mainSidebar"},"foundations/diagrams":{"id":"foundations/diagrams","title":"Conceptual Diagrams for Book Foundations","description":"This file contains descriptions for the conceptual diagrams that should be created for the \"Book Foundations\" section.","sidebar":"mainSidebar"},"foundations/overview":{"id":"foundations/overview","title":"Foundations: Overview","description":"Learning Objectives","sidebar":"mainSidebar"},"foundations/physical_ai":{"id":"foundations/physical_ai","title":"Physical AI: A Primer","description":"Definition","sidebar":"mainSidebar"},"intro":{"id":"intro","title":"Introduction","description":"Welcome to this in-depth guide on Physical AI and Humanoid Robotics. This book is crafted to lead you step by step from the core foundations of robotics and artificial intelligence to advanced, real-world humanoid systems powered by Vision-Language-Action (VLA) models.","sidebar":"mainSidebar"},"pipeline-guide":{"id":"pipeline-guide","title":"Book Content Embeddings Pipeline Guide","description":"Overview","sidebar":"mainSidebar"},"simulation/examples/unity_humanoid_project/README":{"id":"simulation/examples/unity_humanoid_project/README","title":"This file serves as a placeholder for a Unity project for humanoid simulation.","description":"","sidebar":"mainSidebar"},"simulation/gazebo_basics":{"id":"simulation/gazebo_basics","title":"Gazebo Simulation Setup","description":"This chapter will guide you through setting up and using Gazebo, a powerful 3D robot simulator, for simulating humanoid robots. We will cover how to launch Gazebo, import URDF models, and interact with the simulated environment.","sidebar":"mainSidebar"},"simulation/overview":{"id":"simulation/overview","title":"Simulation: Overview","description":"Learning Objectives","sidebar":"mainSidebar"},"simulation/unity_robotics":{"id":"simulation/unity_robotics","title":"Unity Robotics Simulation Setup","description":"This chapter will guide you through setting up and using Unity as a robotics simulation platform. We will cover the installation of Unity, the Unity Robotics Hub, importing URDF models, and basic interaction with ROS 2.","sidebar":"mainSidebar"},"systems/overview":{"id":"systems/overview","title":"Systems: Overview","description":"Learning Objectives","sidebar":"mainSidebar"},"systems/ros2_intro":{"id":"systems/ros2_intro","title":"Introduction to ROS 2","description":"The Robot Operating System (ROS) is a flexible framework for writing robot software. It is a collection of tools, libraries, and conventions that aim to simplify the task of creating complex and robust robot behavior across a wide variety of robotic platforms. ROS 2 is a complete redesign of the original ROS, built to support new use cases like multi-robot systems, real-time control, and small embedded systems.","sidebar":"mainSidebar"},"systems/urdf_control":{"id":"systems/urdf_control","title":"URDF Model Creation and Basics","description":"This chapter provides detailed instructions on how to create a Unified Robot Description Format (URDF) model for a simple humanoid robot. URDF is an XML format used in ROS (Robot Operating System) to describe robot kinematics, dynamics, and visual properties.","sidebar":"mainSidebar"},"tutorial-basics/congratulations":{"id":"tutorial-basics/congratulations","title":"Congratulations!","description":"You have just learned the basics of Docusaurus and made some changes to the initial template.","sidebar":"mainSidebar"},"tutorial-basics/create-a-blog-post":{"id":"tutorial-basics/create-a-blog-post","title":"Create a Blog Post","description":"Docusaurus creates a page for each blog post, but also a blog index page, a tag system, an RSS feed...","sidebar":"mainSidebar"},"tutorial-basics/create-a-document":{"id":"tutorial-basics/create-a-document","title":"Create a Document","description":"Documents are groups of pages connected through:","sidebar":"mainSidebar"},"tutorial-basics/create-a-page":{"id":"tutorial-basics/create-a-page","title":"Create a Page","description":"Add Markdown or React files to src/pages to create a standalone page:","sidebar":"mainSidebar"},"tutorial-basics/deploy-your-site":{"id":"tutorial-basics/deploy-your-site","title":"Deploy your site","description":"Docusaurus is a static-site-generator (also called Jamstack).","sidebar":"mainSidebar"},"tutorial-basics/markdown-features":{"id":"tutorial-basics/markdown-features","title":"Markdown Features","description":"Docusaurus supports Markdown and a few additional features.","sidebar":"mainSidebar"},"tutorial-extras/manage-docs-versions":{"id":"tutorial-extras/manage-docs-versions","title":"Manage Docs Versions","description":"Docusaurus can manage multiple versions of your docs.","sidebar":"mainSidebar"},"tutorial-extras/translate-your-site":{"id":"tutorial-extras/translate-your-site","title":"Translate your site","description":"Let's translate docs/intro.md to French.","sidebar":"mainSidebar"},"vision_language_action/multimodal_interaction":{"id":"vision_language_action/multimodal_interaction","title":"Multimodal Interaction Strategies","description":"This chapter delves into strategies for creating richer and more intuitive human-robot interaction by integrating multiple sensory modalities, primarily vision and natural language. We will explore how combining these inputs can enhance a robot's understanding and response capabilities, focusing on the concept of visual grounding.","sidebar":"mainSidebar"},"vision_language_action/overview":{"id":"vision_language_action/overview","title":"Vision-Language-Action (VLA): Overview","description":"Learning Objectives","sidebar":"mainSidebar"},"vision_language_action/whisper_llm":{"id":"vision_language_action/whisper_llm","title":"Whisper and LLMs for Conversational Robotics","description":"This chapter explores how to integrate state-of-the-art speech-to-text (STT) models like OpenAI Whisper with Large Language Models (LLMs) to create natural and intelligent conversational interfaces for humanoid robots. We will cover the pipeline from spoken command to robot action, focusing on intent understanding and action generation.","sidebar":"mainSidebar"}}}}